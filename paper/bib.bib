% Encoding: UTF-8
%https://libguides.murdoch.edu.au/IEEE/text

@INPROCEEDINGS{4809246,
author={N. {Duhan} and A. K. {Sharma} and K. K. {Bhatia}},
booktitle={2009 IEEE International Advance Computing Conference},
title={Page Ranking Algorithms: A Survey},
year={2009},
volume={},
number={},
pages={1530-1537},
keywords={data mining;information retrieval;Internet;search engines;Web sites;page ranking algorithm;active research;data mining;World Wide Web;hidden information;Web pages;Web server logs;Web content mining;Web structure mining;Web usage mining;search engines;search navigation;Search engines;Web mining;World Wide Web;Data mining;Web pages;Information retrieval;Navigation;Indexing;Web sites;Web server;WWW;Data mining;Web mining;Search engine;Page ranking},
doi={10.1109/IADCC.2009.4809246},
ISSN={null},
month={March}}

@inproceedings{10.1145/775047.775126,
author = {Jeh, Glen and Widom, Jennifer},
title = {SimRank: A Measure of Structural-Context Similarity},
year = {2002},
isbn = {158113567X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775047.775126},
doi = {10.1145/775047.775126},
booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {538–543},
numpages = {6},
location = {Edmonton, Alberta, Canada},
series = {KDD ’02}
}

@ARTICLE{8294302,
author={H. {Cai} and V. W. {Zheng} and K. C. {Chang}},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications},
year={2018},
volume={30},
number={9},
pages={1616-1637},
keywords={data analysis;graph theory;learning (artificial intelligence);graph structural information;graph analytics problem;graph data;graph embedding problem settings;data representation;node classification;node recommendation;link prediction;high computation;space cost;graph properties;formal definition;Taxonomy;Task analysis;Electronic mail;Social network services;Toy manufacturing industry;Two dimensional displays;Machine learning;Graph embedding;graph analytics;graph embedding survey;network embedding},
doi={10.1109/TKDE.2018.2807452},
ISSN={2326-3865},
month={Sep.}}

@article{JAEGER201330,
title = "Type Extension Trees for feature construction and learning in relational domains",
journal = "Artificial Intelligence",
volume = "204",
pages = "30 - 55",
year = "2013",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2013.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S000437021300074X",
author = "Manfred Jaeger and Marco Lippi and Andrea Passerini and Paolo Frasconi",
keywords = "Statistical relational learning, Inductive logic programming, Feature discovery",
abstract = "Type Extension Trees are a powerful representation language for “count-of-count” features characterizing the combinatorial structure of neighborhoods of entities in relational domains. In this paper we present a learning algorithm for Type Extension Trees (TET) that discovers informative count-of-count features in the supervised learning setting. Experiments on bibliographic data show that TET-learning is able to discover the count-of-count feature underlying the definition of the h-index, and the inverse document frequency feature commonly used in information retrieval. We also introduce a metric on TET feature values. This metric is defined as a recursive application of the Wasserstein–Kantorovich metric. Experiments with a k-NN classifier show that exploiting the recursive count-of-count statistics encoded in TET values improves classification accuracy over alternative methods based on simple count statistics."
}

@article{jaeger2019counts,
  title={Counts-of-counts similarity for prediction and search in relational data},
  author={Jaeger, Manfred and Lippi, Marco and Pellegrini, Giovanni and Passerini, Andrea},
  journal={Data mining and knowledge discovery},
  volume={33},
  number={5},
  pages={1254--1297},
  year={2019},
  publisher={Springer}
}

@InProceedings{10.1007/978-981-10-3168-7_23,
author="Shijia, E.
and Jia, Shengbin
and Xiang, Yang
and Ji, Zilian",
editor="Chen, Huajun
and Ji, Heng
and Sun, Le
and Wang, Haixun
and Qian, Tieyun
and Ruan, Tong",
title="Knowledge Graph Embedding for Link Prediction and Triplet Classification",
booktitle="Knowledge Graph and Semantic Computing: Semantic, Knowledge, and Linked Big Data",
year="2016",
publisher="Springer Singapore",
address="Singapore",
pages="228--232",
abstract="The link prediction (LP) and triplet classification (TC) are important tasks in the field of knowledge graph mining. However, the traditional link prediction methods of social networks cannot directly apply to knowledge graph data which contains multiple relations. In this paper, we apply the knowledge graph embedding method to solve the specific tasks with Chinese knowledge base Zhishi.me. The proposed method has been successfully used in the evaluation task of CCKS2016. Hopefully, it can achieve excellent performance.",
isbn="978-981-10-3168-7"
}

@InProceedings{10.1007/978-3-030-34223-4_35,
author="Gao, Kaisheng
and Zhang, Jing
and Zhou, Cangqi",
editor="Cheng, Reynold
and Mamoulis, Nikos
and Sun, Yizhou
and Huang, Xin",
title="Semi-supervised Graph Embedding for Multi-label Graph Node Classification",
booktitle="Web Information Systems Engineering -- WISE 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="555--567",
abstract="The graph convolution network (GCN) is a widely-used facility to realize graph-based semi-supervised learning, which usually integrates node, features, and graph topologic information to build learning models. However, as for multi-label learning tasks, the supervision part of GCN simply minimizes the cross-entropy loss between the last layer outputs and the ground-truth label distribution, which tends to lose some useful information such as label correlations, so that prevents from obtaining high performance. In this paper, we propose a novel GCN-based semi-supervised learning approach for multi-label classification, namely ML-GCN. ML-GCN first uses a GCN to embed the node features and graph topologic information. Then, it randomly generates a label matrix, where each row (i.e., label vector) represents a kind of labels. The dimension of the label vector is the same as that of the node vector before the last convolution operation of GCN. That is, all labels and nodes are embedded in a uniform vector space. Finally, during the model training of ML-GCN, label vectors and node vectors are concatenated to serve as the inputs of the relaxed skip-gram model to detect the node-label correlation as well as the label-label correlation. Experimental results on several graph classification datasets show that the proposed ML-GCN outperforms four state-of-the-art methods.",
isbn="978-3-030-34223-4"
}

@article{lu2015recommender,
  title={Recommender system application developments: a survey},
  author={Lu, Jie and Wu, Dianshuang and Mao, Mingsong and Wang, Wei and Zhang, Guangquan},
  journal={Decision Support Systems},
  volume={74},
  pages={12--32},
  year={2015},
  publisher={Elsevier}
}

@Book{Ricci2015,
  title     = {Recommender Systems Handbook},
  doi       = {10.1007/978-1-4899-7637-6},
  editor    = {Francesco Ricci and Lior Rokach and Bracha Shapira},
  publisher = {Springer {US}},
  year      = {2015},
}

@online{Grouplensdata,
    author = "Grouplens",
    title = "Grouplens: MovieLens Data",
    url  = "https://grouplens.org/datasets/movielens/",
    addendum = "(accessed: 07.04.2020)",
    keywords = "grouplens,movielens,data"
}

@article{singh2013k,
	title={K-means with Three different Distance Metrics},
	author={Singh, Archana and Yadav, Avantika and Rana, Ajay},
	journal={International Journal of Computer Applications},
	volume={67},
	number={10},
	year={2013},
	publisher={Citeseer}
}