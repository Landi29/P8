% Encoding: UTF-8
%https://libguides.murdoch.edu.au/IEEE/text


@article{uhlmann1991,
  title={Satisfying general proximity/similarity queries with metric trees},
  author={Uhlmann, Jeffrey K},
  journal={Information processing letters},
  volume={40},
  number={4},
  pages={175--179},
  year={1991},
  publisher={Elsevier}
}


@INPROCEEDINGS{recommender_e-comerce,
author={S. {Sivapalan} and A. {Sadeghian} and H. {Rahnama} and A. M. {Madni}},
booktitle={2014 World Automation Congress (WAC)},
title={Recommender systems in e-commerce},
year={2014},
volume={},
number={},
pages={179-184}}


@INPROCEEDINGS{4809246,
author={N. {Duhan} and A. K. {Sharma} and K. K. {Bhatia}},
booktitle={2009 IEEE International Advance Computing Conference},
title={Page Ranking Algorithms: A Survey},
year={2009},
volume={},
number={},
pages={1530-1537},
keywords={data mining;information retrieval;Internet;search engines;Web sites;page ranking algorithm;active research;data mining;World Wide Web;hidden information;Web pages;Web server logs;Web content mining;Web structure mining;Web usage mining;search engines;search navigation;Search engines;Web mining;World Wide Web;Data mining;Web pages;Information retrieval;Navigation;Indexing;Web sites;Web server;WWW;Data mining;Web mining;Search engine;Page ranking},
doi={10.1109/IADCC.2009.4809246},
ISSN={null},
month={March}}

@inproceedings{10.1145/775047.775126,
author = {Jeh, Glen and Widom, Jennifer},
title = {SimRank: A Measure of Structural-Context Similarity},
year = {2002},
isbn = {158113567X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775047.775126},
doi = {10.1145/775047.775126},
booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {538–543},
numpages = {6},
location = {Edmonton, Alberta, Canada},
series = {KDD ’02}
}

@ARTICLE{8294302,
author={H. {Cai} and V. W. {Zheng} and K. C. {Chang}},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications},
year={2018},
volume={30},
number={9},
pages={1616-1637},
keywords={data analysis;graph theory;learning (artificial intelligence);graph structural information;graph analytics problem;graph data;graph embedding problem settings;data representation;node classification;node recommendation;link prediction;high computation;space cost;graph properties;formal definition;Taxonomy;Task analysis;Electronic mail;Social network services;Toy manufacturing industry;Two dimensional displays;Machine learning;Graph embedding;graph analytics;graph embedding survey;network embedding},
doi={10.1109/TKDE.2018.2807452},
ISSN={2326-3865},
month={Sep.}}

@article{JAEGER201330,
title = "Type Extension Trees for feature construction and learning in relational domains",
journal = "Artificial Intelligence",
volume = "204",
pages = "30 - 55",
year = "2013",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2013.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S000437021300074X",
author = "Manfred Jaeger and Marco Lippi and Andrea Passerini and Paolo Frasconi",
keywords = "Statistical relational learning, Inductive logic programming, Feature discovery",
abstract = "Type Extension Trees are a powerful representation language for “count-of-count” features characterizing the combinatorial structure of neighborhoods of entities in relational domains. In this paper we present a learning algorithm for Type Extension Trees (TET) that discovers informative count-of-count features in the supervised learning setting. Experiments on bibliographic data show that TET-learning is able to discover the count-of-count feature underlying the definition of the h-index, and the inverse document frequency feature commonly used in information retrieval. We also introduce a metric on TET feature values. This metric is defined as a recursive application of the Wasserstein–Kantorovich metric. Experiments with a k-NN classifier show that exploiting the recursive count-of-count statistics encoded in TET values improves classification accuracy over alternative methods based on simple count statistics."
}

@article{jaeger2019counts,
  title={Counts-of-counts similarity for prediction and search in relational data},
  author={Jaeger, Manfred and Lippi, Marco and Pellegrini, Giovanni and Passerini, Andrea},
  journal={Data mining and knowledge discovery},
  volume={33},
  number={5},
  pages={1254--1297},
  year={2019},
  publisher={Springer}
}

@InProceedings{10.1007/978-981-10-3168-7_23,
author="Shijia, E.
and Jia, Shengbin
and Xiang, Yang
and Ji, Zilian",
editor="Chen, Huajun
and Ji, Heng
and Sun, Le
and Wang, Haixun
and Qian, Tieyun
and Ruan, Tong",
title="Knowledge Graph Embedding for Link Prediction and Triplet Classification",
booktitle="Knowledge Graph and Semantic Computing: Semantic, Knowledge, and Linked Big Data",
year="2016",
publisher="Springer Singapore",
address="Singapore",
pages="228--232",
abstract="The link prediction (LP) and triplet classification (TC) are important tasks in the field of knowledge graph mining. However, the traditional link prediction methods of social networks cannot directly apply to knowledge graph data which contains multiple relations. In this paper, we apply the knowledge graph embedding method to solve the specific tasks with Chinese knowledge base Zhishi.me. The proposed method has been successfully used in the evaluation task of CCKS2016. Hopefully, it can achieve excellent performance.",
isbn="978-981-10-3168-7"
}

@InProceedings{10.1007/978-3-030-34223-4_35,
author="Gao, Kaisheng
and Zhang, Jing
and Zhou, Cangqi",
editor="Cheng, Reynold
and Mamoulis, Nikos
and Sun, Yizhou
and Huang, Xin",
title="Semi-supervised Graph Embedding for Multi-label Graph Node Classification",
booktitle="Web Information Systems Engineering -- WISE 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="555--567",
abstract="The graph convolution network (GCN) is a widely-used facility to realize graph-based semi-supervised learning, which usually integrates node, features, and graph topologic information to build learning models. However, as for multi-label learning tasks, the supervision part of GCN simply minimizes the cross-entropy loss between the last layer outputs and the ground-truth label distribution, which tends to lose some useful information such as label correlations, so that prevents from obtaining high performance. In this paper, we propose a novel GCN-based semi-supervised learning approach for multi-label classification, namely ML-GCN. ML-GCN first uses a GCN to embed the node features and graph topologic information. Then, it randomly generates a label matrix, where each row (i.e., label vector) represents a kind of labels. The dimension of the label vector is the same as that of the node vector before the last convolution operation of GCN. That is, all labels and nodes are embedded in a uniform vector space. Finally, during the model training of ML-GCN, label vectors and node vectors are concatenated to serve as the inputs of the relaxed skip-gram model to detect the node-label correlation as well as the label-label correlation. Experimental results on several graph classification datasets show that the proposed ML-GCN outperforms four state-of-the-art methods.",
isbn="978-3-030-34223-4"
}

@article{lu2015recommender,
  title={Recommender system application developments: a survey},
  author={Lu, Jie and Wu, Dianshuang and Mao, Mingsong and Wang, Wei and Zhang, Guangquan},
  journal={Decision Support Systems},
  volume={74},
  pages={12--32},
  year={2015},
  publisher={Elsevier}
}

@Book{Ricci2015,
  title     = {Recommender Systems Handbook},
  doi       = {10.1007/978-1-4899-7637-6},
  editor    = {Francesco Ricci and Lior Rokach and Bracha Shapira},
  publisher = {Springer {US}},
  year      = {2015},
}

@online{Grouplensdata,
    author = "Grouplens",
    title = "Grouplens: MovieLens Data",
    url  = "https://grouplens.org/datasets/movielens/",
    addendum = "(accessed: 07.04.2020)",
    keywords = "grouplens,movielens,data"
}

@article{singh2013k,
	title={K-means with Three different Distance Metrics},
	author={Singh, Archana and Yadav, Avantika and Rana, Ajay},
	journal={International Journal of Computer Applications},
	volume={67},
	number={10},
	year={2013},
	publisher={Citeseer}
}

@article{chai2014root,
	title={Root mean square error (RMSE) or mean absolute error (MAE)?--Arguments against avoiding RMSE in the literature},
	author={Chai, Tianfeng and Draxler, Roland R},
	journal={Geoscientific model development},
	volume={7},
	number={3},
	pages={1247--1250},
	year={2014},
	publisher={Copernicus GmbH}
}

@Article{Bai2018,
  author      = {Yunsheng Bai and Hao Ding and Song Bian and Ting Chen and Yizhou Sun and Wei Wang},
  date        = {2018-08-16},
  title       = {SimGNN: A Neural Network Approach to Fast Graph Similarity Computation},
  eprint      = {1808.05689v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Graph similarity search is among the most important graph-based applications, e.g. finding the chemical compounds that are most similar to a query compound. Graph similarity computation, such as Graph Edit Distance (GED) and Maximum Common Subgraph (MCS), is the core operation of graph similarity search and many other applications, but very costly to compute in practice. Inspired by the recent success of neural network approaches to several graph applications, such as node or graph classification, we propose a novel neural network based approach to address this classic yet challenging graph problem, aiming to alleviate the computational burden while preserving a good performance. The proposed approach, called SimGNN, combines two strategies. First, we design a learnable embedding function that maps every graph into a vector, which provides a global summary of a graph. A novel attention mechanism is proposed to emphasize the important nodes with respect to a specific similarity metric. Second, we design a pairwise node comparison method to supplement the graph-level embeddings with fine-grained node-level information. Our model achieves better generalization on unseen graphs, and in the worst case runs in quadratic time with respect to the number of nodes in two graphs. Taking GED computation as an example, experimental results on three real graph datasets demonstrate the effectiveness and efficiency of our approach. Specifically, our model achieves smaller error rate and great time reduction compared against a series of baselines, including several approximation algorithms on GED computation, and many existing graph neural network based models. To the best of our knowledge, we are among the first to adopt neural networks to explicitly model the similarity between two graphs, and provide a new direction for future research on graph similarity computation and graph similarity search.},
  file        = {:http\://arxiv.org/pdf/1808.05689v4:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Kipf2016,
  author      = {Thomas N. Kipf and Max Welling},
  date        = {2016-09-09},
  title       = {Semi-Supervised Classification with Graph Convolutional Networks},
  eprint      = {1609.02907v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  file        = {:http\://arxiv.org/pdf/1609.02907v4:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Node2vec,
  author    = {Aditya Grover and
               Jure Leskovec},
  title     = {node2vec: Scalable Feature Learning for Networks},
  journal   = {CoRR},
  volume    = {abs/1607.00653},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.00653},
  archivePrefix = {arXiv},
  eprint    = {1607.00653},
  timestamp = {Mon, 13 Aug 2018 16:48:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GroverL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{Word2vec,
    author = "Chris McCormik",
    title = "Word2vec Tutorial - The Skip-gram Model",
    url  = "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/",
    addendum = "(accessed: 09.05.2020)",
    keywords = "Word2vec,skip-gram"
}

@online{Gensim.Word2vec,
    author = "gensim",
    title = "models.word2vec – Word2vec embeddings",
    url  = "https://radimrehurek.com/gensim/models/word2vec.html",
    addendum = "(accessed: 14.05.2020)",
    keywords = "Word2vec,gensim"
}

@online{n2v.eliorc,
    author = "eliorc",
    title = "Implementation of the node2vec algorithm",
    url  = "https://github.com/eliorc/node2vec",
    addendum = "(accessed: 14.05.2020)",
    keywords = "node2vec,implementation"
}

@inproceedings{saveski2014item,
	title={Item cold-start recommendations: learning local collective embeddings},
	author={Saveski, Martin and Mantrach, Amin},
	booktitle={Proceedings of the 8th ACM Conference on Recommender systems},
	pages={89--96},
	year={2014}
}
@InCollection{Riesen2015,
  author    = {Kaspar Riesen},
  booktitle = {Structural Pattern Recognition with Graph Edit Distance},
  title     = {Graph Edit Distance},
  doi       = {10.1007/978-3-319-27252-8_2},
  pages     = {29--44},
  publisher = {Springer International Publishing},
  year      = {2015},
}

@Comment{jabref-meta: databaseType:biblatex;}
