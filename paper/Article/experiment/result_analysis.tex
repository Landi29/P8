\subsection{Result analysis}\label{Subsec:results}
The results of each test will in this section be compared to the baseline RMSE as seen in \autoref{fig:base_errors}, and with the RMSE score of the different methods.


\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/base_10_fold_error.tex}
	\end{adjustbox}
	\caption{base error from 100k dataset }
	\label{fig:base_errors}
\end{figure}

In \autoref{fig:base_errors} we have the baseline RMSE, we got this by using the average rating for the dataset as the prediction for all future ratings.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/brute_10_fold_error.tex}
	\end{adjustbox}
	\caption{bruteforce error from 100k dataset }
	\label{fig:brute_errors}
\end{figure}

\autoref{fig:brute_errors} show the RMSE for the 20 evaluations. 
This shows that with a relatively simple method, recommendations can be done better than the baseline.
\autoref{fig:brute_errors} also shows that the RMSE in most folds only have a difference of $0.01$ between validation and test sets.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/N2V_10_fold_error.tex}
	\end{adjustbox}
	\caption{Node2vec error from 100k dataset}
	\label{fig:N2V_errors}
\end{figure}

The graph in \autoref{fig:N2V_errors} shows the most promising of our results with RMSE values being as low as 0.94 . By tweaking the metadata of the training we believe that there are potential for even better recommendations.
The discrepancy between validation and test in fold 7 seems to indicate that this fold has been fitted wrongly.


\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/tet_10_fold_error.tex}
	\end{adjustbox}
	\caption{TET error from 100k dataset }
	\label{fig:tet_errors}
\end{figure}


The RMSE scores for the TETs seen in \autoref{fig:tet_errors}, are at a glance not impressive, but the RMSE scores are still better than the baseline. 
This shows potential in the framework and it might be better with more descriptive features. The advantage of recommendation with TETs, is that it can depend only on structural information.
Where the other methods we have discussed are dependent on the nearest neighbors having rated a product or movie to make a recommendation, the TETs only need movies or products that look like the movie or product you try to predict a recommendation for.

A scenario where the TETs structure should be better is with the addition of new items in the database.
We made this scenario by removing any relation to all the movies in the verification and test sets from all users in the traning set, and making the predictions for a single person.
The error scores will because of this be far more fluctuating.
We ran the scenario on the TETs and Node2vec.
The Node2vec's predictions should default to the users average rating since there are no other users that have rated the movies. As a consequence, we do not have their input to the prediction.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/tet_scenario_10_fold_error.tex}
	\end{adjustbox}
	\caption{TET scenario error from 100k dataset}
	\label{fig:tet_scenario_errors}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/N2V_scenario_10_fold_error.tex}
	\end{adjustbox}
	\caption{Node2vec scenario error from 100k dataset}
	\label{fig:N2V_scenario_errors}
\end{figure}

From this scenario we can clearly see that the TET comparison and predictions (\autoref{fig:tet_scenario_errors}), are better than the predictions made by Node2Vec (\autoref{fig:N2V_scenario_errors}).
Fold 4 for the in \autoref{fig:N2V_scenario_errors} is the best trained for the scenario.
This fold shows a difference between validation and test of 0.03 but at the lowest it has a RMSE at 1.08.
The RMSE for the TET predictions  as shown in \autopageref{fig:tet_scenario_errors} only has two folds were either validation or test has a RMSE of more than 1.
All other folds has a RMSE below one, but the predictions has some uncertainty between validation and test.


Looking at the runtime of the different methods during the tests, we found the average time it takes to run pr. person.
Bruteforce had an average runtime of 0.18 seconds.
The node2vec had an average runtime of 0.005 seconds.
The TET comparisons had an average of 5.02 seconds.

Looking at the different runtimes we see that node2vec is the fastest recommender and that the TETs takes 1000 times longer to make a recommendation this could potentially be improved by implementing a matrix containing the similarities, instead of calculating them for each user in runtime.
