\subsection{Result analysis}\label{Subsec:results}
The results of each test will in this section be compared to the baseline RMSE as seen in \autoref{fig:base_errors}, and with the RMSE score of the different methods.


\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/base_10_fold_error.tex}
	\end{adjustbox}
	\caption{base error from 100k dataset }
	\label{fig:base_errors}
\end{figure}

In \autoref{fig:base_errors} we have the baseline RMSE, we got this by using the average rating for the dataset as the prediction for all future ratings.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/brute_10_fold_error.tex}
	\end{adjustbox}
	\caption{bruteforce error from 100k dataset }
	\label{fig:brute_errors}
\end{figure}

\autoref{fig:brute_errors} show the RMSE for the 20 evaluations. 
This shows that with a relatively simple method, recommendations can be done better than the baseline.
\autoref{fig:brute_errors} also shows that the RMSE in most folds only have a difference of $0.01$ between validation and test sets.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/N2V_10_fold_error.tex}
	\end{adjustbox}
	\caption{Node2vec error from 100k dataset}
	\label{fig:N2V_errors}
\end{figure}

The graph in \autoref{fig:N2V_errors} shows the most promising of our results with RMSE values being as low as 0.94 . By tweaking the metadata of the training we believe that there are potential for even better recommendations.
The discrepancy between validation and test in fold 7 seems to indicate that this fold has been fitted wrongly.


\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/tet_10_fold_error.tex}
	\end{adjustbox}
	\caption{TET error from 100k dataset }
	\label{fig:tet_errors}
\end{figure}


The RMSE scores for the TETs seen in \autoref{fig:tet_errors}, are at a glance not impressive, but the RMSE scores are still better than the baseline.

This shows potential in the framework and it might be better with more descriptive features. The advantage of recommendation with TETs, is that it can depend only on structural information.
Where the other methods we have discussed are dependent on the nearest neighbors having rated a product or movie to make a recommendation, the TETs only need movies or products that look like the movie or product you try to predict a recommendation for.

A scenario where the TETs structure should be better is with the addition of new items in the database.
We made this scenario by removing any relation to all the movies in the verification and test sets from all users in the traning set.
As a consequence, we do not have their input to the prediction.
We ran the scenario on the TETs and Node2vec.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/abstract_base_10_fold_error.tex}
	\end{adjustbox}
	\caption{abstract baseline error from 100k dataset}
	\label{fig:abstract_baseline_errors}
\end{figure}

Because the TET rating set is split in 3 instead of 5, when making recommendation based on subTETs we are able to recommend within this set.
We abstracted the ratings to this form for the baseline as seen in \autoref{fig:abstract_baseline_errors}.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/abstract_N2V_scenario_10_fold_error.tex}
	\end{adjustbox}
	\caption{Node2vec scenario error from 100k dataset}
	\label{fig:N2V_scenario_errors}
\end{figure}

The predictions from node2vec default to the users' average rating since there are no other users that have rated the movies.
When the predictions are calculated they are abstracted similarly to the baseline the RMSE can be seen in \autoref{fig:N2V_scenario_errors}. 
These predictions are still better than baseline because they consider the user's average rating rather than the datasets' average rating.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/tet_scenario_10_fold_error.tex}
	\end{adjustbox}
	\caption{TET scenario error from 100k dataset}
	\label{fig:tet_scenario_errors}
\end{figure}

From the $3$ figures including \autoref{fig:tet_scenario_errors} we can clearly see that the TET comparison and predictions.
With a average RMSE for the ratings at around 0.79, close to 0.15 points better than the average for node2vec.

As seen in \autoref{fig:N2V_errors}, \ref{fig:brute_errors} and \ref{fig:tet_errors} the best of the comparison models with the best RMSE scores is node2vec. 
