\subsection{Result analysis}\label{Subsec:results}
The results of each test will in this section be compared to the baseline RMSE as seen in \autoref{fig:base_errors}, and with the RMSE score of the different methods.


\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/base_10_fold_error.tex}
	\end{adjustbox}
	\caption{base error from 100k dataset }
	\label{fig:base_errors}
\end{figure}

In \autoref{fig:base_errors} we have the baseline RMSE, we got this by using the average rating for the dataset as the prediction for all future ratings.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/brute_10_fold_error.tex}
	\end{adjustbox}
	\caption{bruteforce error from 100k dataset }
	\label{fig:brute_errors}
\end{figure}

\autoref{fig:brute_errors} show the RMSE for the 20 evaluations. 
This shows that with a relatively simple method, recommendations can be done better than the baseline.
\autoref{fig:brute_errors} also shows that the RMSE in most folds only have a difference of $0.01$ between validation and test sets.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/N2V_10_fold_error.tex}
	\end{adjustbox}
	\caption{Node2vec error from 100k dataset}
	\label{fig:N2V_errors}
\end{figure}

The graph in \autoref{fig:N2V_errors} shows the most promising of our results with RMSE values being as low as 0.94 . By tweaking the metadata of the training we believe that there are potential for even better recommendations.
The discrepancy between validation and test in fold 7 seems to indicate that this fold has been fitted wrongly.


\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/tet_10_fold_error.tex}
	\end{adjustbox}
	\caption{TET error from 100k dataset }
	\label{fig:tet_errors}
\end{figure}


The RMSE scores for the TETs seen in \autoref{fig:tet_errors}, are at a glance not impressive, but the RMSE scores are still better than the baseline.

This shows potential in the framework and it might be better with more descriptive features. The advantage of recommendation with TETs, is that it can depend only on structural information.
Where the other methods we have discussed are dependent on the nearest neighbors having rated a product or movie to make a recommendation, the TETs only need movies or products that look like the movie or product you try to predict a recommendation for.

A scenario where the TETs structure should be better is with the addition of new items in the database where you have an item cold start problem. The cold start problem is a well known problem for recommenders\cite{Ricci2015}\cite{saveski2014item}.
The problem occurs when a data set is inadequate, leading to recommendation problems.
There are a few different versions of the cold start problem, and the item cold start problem is one of these. 
Item cold start is a problem that occurs with new data sets or when new items are added to the data set.
When new data is added to the data set, there are no relations between users and newly added items.
This fact makes it near impossible to make reliable recommendations.
The TETs' predictions can work around the item cold start problem by finding structurally similar items and using their ratings in the prediction calculation.

We set up this scenario by removing any relation to all the movies in the verification and test sets from all users in the training set.
As a consequence, we do not have their input to the prediction.
We ran this scenario on the TETs and node2vec.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/abstract_base_10_fold_error.tex}
	\end{adjustbox}
	\caption{abstract baseline error from 100k dataset}
	\label{fig:abstract_baseline_errors}
\end{figure}

Because the TET rating set is split in 3 instead of 5, when making recommendation based on subTETs we are able to recommend within this set.
We abstracted the ratings to this form for the baseline as seen in \autoref{fig:abstract_baseline_errors}.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/abstract_N2V_scenario_10_fold_error.tex}
	\end{adjustbox}
	\caption{Node2vec scenario error from 100k dataset}
	\label{fig:N2V_scenario_errors}
\end{figure}

The predictions from node2vec default to the users' average rating since there are no other users that have rated the movies.
When the predictions are calculated they are abstracted similarly to the baseline the RMSE can be seen in \autoref{fig:N2V_scenario_errors}. 
These predictions are still better than baseline because they consider the user's average rating rather than the datasets' average rating.

\begin{figure}[H]
	\centering
	\begin{adjustbox}{width=0.5\textwidth}
		\input{Article/figures/tet_scenario_10_fold_error.tex}
	\end{adjustbox}
	\caption{TET scenario error from 100k dataset}
	\label{fig:tet_scenario_errors}
\end{figure}

From the $3$ figures including \autoref{fig:tet_scenario_errors} we can clearly see that the TET comparison and predictions.
With a average RMSE for the ratings at around 0.79, close to 0.15 points better than the average for node2vec.

Looking at the runtime of the different methods during the tests, we found the average time it takes to run pr. person.
Bruteforce had an average runtime of 0.18 seconds.
The node2vec had an average runtime of 0.005 seconds.
The TET comparisons had an average of 5.02 seconds.

Looking at the different runtimes we see that node2vec is fastest at making recommendations and that the TETs takes around 1000 times as long to make a recommendation.
A reason for this difference is that the node2vec model is trained and stored in memory, making it faster and easier to look up the vectors and compute the similarities.
The TET runtime could potentially be improved by pre-computing the values, implementing a matrix containing the similarities, so that you can look up the result instead of calculating them for each user in runtime.
