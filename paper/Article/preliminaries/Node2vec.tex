\subsection{Node2vec}
In this paper we will look at graph embedding as a way to find structural similarity between nodes in a graph, specifically node embedding. This is done using the implementation in “Node2vec: Scalable Feature Learning for Networks” by Aditya Grover and Jure Leskovec. They present an algorithm called node2vec for efficient feature learning for nodes in networks. This is done by mapping nodes to a vector representation of their features, that try to preserve local networks of neighborhood nodes and communities, using a random walk approach to generate a nodes neighborhood network. They formulate a random walk strategy that interpolates between BFS and DFS to try and preserve both homophily and structural equivalence.

Node2vec makes use of word2vec skip-gram model to train a model for embedding the nodes, thus an overall understanding of word2vec is needed.

The skip-gram model trains a neural network with a single hidden layer, but instead of using the model for what it was trained for, you discard the model and keep the weights of the hidden layer. These weights are then used as our vectors. The model is trained by giving it a specific word in a sentence, looking at a window around it and picking a random word within the window. 