\subsection{Node2vec}
In this paper we will look at graph embedding as a way to find structural similarity between nodes in a graph, specifically node embedding. This is done using the implementation in “Node2vec: Scalable Feature Learning for Networks” by Aditya Grover and Jure Leskovec. They present an algorithm called node2vec for efficient feature learning for nodes in networks. This is done by mapping nodes to a vector representation of their features, that try to preserve local networks of neighborhood nodes and communities, using a random walk approach to generate a nodes neighborhood network. They formulate a random walk strategy that interpolates between BFS and DFS to try and preserve both homophily and structural equivalence.

Node2vec makes use of word2vec skip-gram model to train a model for embedding the nodes, thus an overall understanding of word2vec is needed.

The idea behind the skip-gram model is that given a specific word in a sentence, look at a so called window around it, and pick a random word within the window. As output we get for each word, the probability for that word being the randomly chosen one.

The model is trained by feeding it word pairs from sentences, sliding the window across and picking random words, training the model on how often each word pairing shows up. An example can be seen in 

\TODO tilføj figure omkring word pairs og indsæt ref

This is done by first creating a vocabulary $V$, that consist of all unique words found in our sentences. The input-layer will then be a single word represented as a one-hot encoding of size $V$. The size of the hidden layer is the number of features or dimensions $D$ we want to learn. Lastly the output layer is a vector of size $V$, containing the probability for each word in our vocabulary being the randomly chosen word, or how related each word is to the input word.

\TODO indsæt eksempel på neural network

Once training is done, instead of using the model for what it was trained for, the model is discarded and only the weights of the hidden layer is kept. These weights are then what is used as our vectors. Giving us $D$ vectors of size $V$ 

Text have a linear nature in form of sentences, but networks are not linear and thus a richer notion of a neighborhood is needed.

\TODO Description of a graph network and what a neighborhood is

\TODO desciption of the sampling strategy including p and q parameters