\subsection{Node2vec}
In this paper we will look at graph embedding as a way to find structural similarity between nodes in a graph, specifically node embedding. This is done using the implementation in “Node2vec: Scalable Feature Learning for Networks” by Aditya Grover and Jure Leskovec. They present an algorithm called node2vec for efficient feature learning for nodes in networks. This is done by mapping nodes to a vector representation of their features, that try to preserve local networks of neighborhood nodes and communities, using a random walk approach to generate a nodes neighborhood network. They formulate a random walk strategy that interpolates between BFS and DFS to try and preserve both homophily and structural equivalence.

Node2vec makes use of word2vec skip-gram model to train a model for embedding the nodes, thus an overall understanding of word2vec is needed.

The idea behind the skip-gram model is that given a specific word in a sentence, look at a so called window around it, and pick a random word within the window. As output we get for each word, the probability for that word being the randomly chosen one. That means we as output get the probability for the words being related.

Quickly describe how it is trained
example of window

\TODO example of the neural network

But instead of using the model for what it was trained for, you discard the model and keep the weights of the hidden layer. These weights are then what is used as our vectors.

Text have a linear nature in form of sentences, but networks are not linear and thus a richer notion of a neighborhood is needed.

