\subsection{Node2Vec and the size of input data}\label{AP:N2V}
% This part may need revisioning:
As stated in section (section\_reference), embdedding the Movielens dataset with Node2Vec will use a lot of memory, and thus not a very feasible solution. We narrowed the issue down to the call of the function  \textit{preprocess\_transition\_probs} (\textit{ptp}) from the implementation based on the paper \cite{Node2vec}. The function aims to create a preliminary transition probability matrix. The dependence on this matrix however, makes Node2Vec difficult to use with big datasets that have tens of millions of edges. In order to test how much memory that would be necessary in order to embed the Movielens dataset with 25 million rankings, we have analyzed the \textit{ptp} function of the Node2Vec library and executed several experiments in order to derive memory usage estimates of the algorithm. In the following, we will discuss Node2Vec's use of networkx and the \textit{ptp} function.
% The results:
The first bottleneck of the Node2Vec library when it comes to very large graphs, is its use of the networkx library and the networkx graph datastructure. The class Graph provided by networkx is in many ways very useful and provides several tools for creating and working with different types of graphs. Furthermore, networkx tries to speed up common graph traversal tasks and calculations by adding metadata to the Graph object in memory, thus saving on intermediate calculations. This approach however, becomes problematic when working with large graphs with millions of edges since it costs a lot of main memory. The size of our data file, graph.csv, is about 460 megabytes. But when loading it into a networkx Graph, the corresponding Python process uses about 15 GB of main memory. By creating a simple Graph class in both Python 3.7 and C++ that only holds a list of nodes and a list of edges, we have constructed graphs with the same dimensions as the Movielens graph and analyzed their memory usage, for which the results can be seen in table 1.

 As seen in table 1, Python requires about 2.8 GB of memory to construct the graph, where the list of nodes and the list of edges require a total of 2.4 GB. Reading the same data into a graph object in C++ however, only require about 0.299 GB or 299 MB. We see here that networkx uses the most memory when loading the graph.

% Analysis of the preprocess_transition_probs() function:
The memory allocation of the \textit{ptp} function mainly takes place in two loops, one for each node and one for each edge. In these loops, values are stored to two corresponding dictionaries: alias\_nodes and alias\_edges.
