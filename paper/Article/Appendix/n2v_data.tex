\subsection{Node2Vec and the size of input data}\label{AP:N2V}
% This part may need revisioning:
As stated in section (section\_reference), embdedding the Movielens dataset with Node2Vec will use a lot of memory, and thus not a very feasible solution. We narrowed the issue down to the call of the function  \textit{preprocess\_transition\_probs} (\textit{ptp}) from the implementation based on the paper \cite{Node2vec}. The function aims to create a preliminary transition probability matrix. The dependence on this matrix however, makes Node2Vec difficult to use with big datasets that have tens of millions of edges. In order to test how much memory that would be necessary in order to embed the Movielens dataset with 25 million rankings, we have analyzed the \textit{ptp} function of the Node2Vec library and calculated memory usage estimates of the algorithm. In the following, we will discuss Node2Vec's use of networkx and the \textit{ptp} function.
% The results:
The first bottleneck of the Node2Vec library when it comes to very large graphs, is its use of the networkx library and the networkx Graph datastructure.
The class Graph provided by networkx is in many ways very useful and provides several tools for creating and working with different types of graphs. Furthermore, networkx tries to speed up common graph traversal tasks and calculations by adding metadata to the Graph object in memory, thus saving on intermediate calculations.
This approach however, becomes problematic when working with large graphs consisting of millions of edges. The size of our data file, graph.csv, is about 460 megabytes. But when loading it into a networkx Graph, the corresponding Python process uses about 15 GB of main memory in order to represent the same graph. This results in an increase of memory usage with a scale of 32, when loading the graph from disc to main memory.

A solution to this problem could be to represent the graph as a sparse adjacency matrix  in a dictionary like structure as shown in \autoref{table:sparse_matrix_1}. The weights between nodes could then be saved in a separate dictionary structure as in \autoref{table:sparse_matrix_weights} . An even more compact solution could be to store the weights in the adjacency matrix, as in table3.
\input{Article/Appendix/matrix_tables.tex}




%table 1:
%[key = node\_id, value = [list\_of\_connected\_node\_ids]].
%table 2:
%[key = $(node_1, \, node_2)$, value = weight]
%table3:
%adding values of the form [(n_1,w_1), (n_2, w_2), (n_n, w_n)] in the form [key = node\_id, value = [tuples of connected nodes and their weights]].

% Analysis of the preprocess_transition_probs() function:
The memory allocation of the \textit{ptp} function mainly takes place in two loops, one for each node and one for each edge. In these loops, values are stored to two corresponding dictionaries: alias\_nodes and alias\_edges.
