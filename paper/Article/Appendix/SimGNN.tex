\subsection{SimGNN}\label{AP:SimGNN}
One of the methods that we tried to use was Similarity Computation via Graph Neural Networks or SimGNN which is an algorithm used for calculating the similarity between two graphs created by Bai et al. \cite{Bai2018}. The algorithm was intended to be utilized to compare two users from the Movielens dataset but was found to be too complex when it comes to finding Graph Edit Distance (GED). This was discovered upon running through the test set after SimGNN\footnote{The original code we used for computing SimGNN can be found here: https://github.com/benedekrozemberczki/SimGNN} had been trained where the mean squared error results turned out to be 0.023. While these result are in themselves are very good it made us suspicion that the result might be too good. After further investigation we concluded that our graphs which SimGNN trains on are supposed to predict are very simple and that SimGNN therefore isn't suited for our use case. Combine this with the fact that SimGNN take about 2 days to train on our dataset and that the GED for two graphs from our dataset can be computed with a simpler and faster implementation only reinforces this notion. The remainder of this section is a explanation on how SimGNN works.

The algorithm works by using two different strategies. The first primary strategy is to compute the similarity between two graphs based on interaction between their graph level embeddings. The second auxiliary strategy computes similarity based on two sets of node-level embeddings. The second strategy is optional and only helps to give a more accurate prediction at the cost of runtime\cite{Bai2018}.

Both strategies in SimGNN require that the graphs get embedded at the node level. To perform this embedding Bai et. al. uses a Graph Convolutional Network (GCN)\cite{Bai2018} which is created by Kipf et. al. \cite{Kipf2016}. The main operation behind GCN in SimGNN is shown in \autoref{Eq:GCN}.


\begin{equation}\label{Eq:GCN}
conv(u_n)=f_1(\sum_{m \in N(n)} \frac{1}{\sqrt{d_nd_m}}u_mW_1^{(l)}+b_1^{(l)})
\end{equation}

In \autoref{Eq:GCN} $u_n$ is a representation of a node. $N(n)$ is the set of first-order neighbors of a node $n$ plus $n$ itself, $d_n$ is the degree of node $n$ plus 1, $w_1^{(l)}$ is a weight matrix associated l-th GCN layer, $b_1^{(l)}$ is the bias and $f_1()$ is an activation function. The idea of \autoref{Eq:GCN} is that it aggregates features from the first-order neighbor of a node $n$\cite{Bai2018}.

What is happening inside GCN is the following. Given a graph $G$ the GCN f(X,A) takes as input: a feature matrix $X$ and an adjacency matrix $A$ which represents G. Inside each hidden layer in the GCN the following propagation takes place\cite{Kipf2016}:



\begin{equation}
\label{eq:propagation_rule}
H^{l+1} = \sigma (\tilde{D}^{-0.5}*\tilde{A}*\tilde{D}^{0.5}*H{(l)} * W^{(l)})
\end{equation}

In \autoref{eq:propagation_rule} $\tilde{A} = A +I_N$ is the adjacency matrix of the graph with an added self-loop which is the identity matrix $I_N$. $\tilde{D} = \sum_j * \tilde{A}_ij$ is the degree matrix which is computed by summing up the feature representation of the neighbors of $\tilde{A}$. The reason why $\tilde{D}$ is to the power of $^{-0.5}$ is to avoid vanishing/exploding gradients. $H^{(l)}$ is the matrix of activations from the previous l'th layer with $H^0$ representing the input layer, thereby being the feature matrix X. $W^{(l)}$ is the weigh matrix from the previous l'th layer and lastly $\sigma$ represents an activation function such as ReLU\cite{Kipf2016}. To try and connect \autoref{eq:propagation_rule} with \autoref{Eq:GCN} is that nodes in the graph ends up being represented as the sum of the features from the neighbor node and the current node itself multiplied with a weight and a bias and put into an activation function.


Once node-level embedding has been computed, the first strategy can use these to compute the graph-level embeddings. Here Bai et. al. present an attention mechanism that is used to figure out which nodes are more important than others with respect to a specific similarity metric. Consequently more important nodes should therefore receive higher weights\cite{Bai2018}. The equation for the attention mechanism can be seen \autoref{Eq:Att}.



\begin{equation}\label{Eq:Att}
h= \sum^N_{n=1} \sigma(u^T_nc)u_n=\sum^N_{n=1}\sigma(u^T_ntanh((\frac{1}{N}\sum_{m=1}^Nu_n)W_2))u_n
\end{equation}

The idea behind \autoref{Eq:Att} is that when creating a graph embedding $h$ we compute a global graph context $c$ which is done by taking the average of all the node-level embedding and feeding them into a nonlinear transformation function. This means that the global context $c=tanh((\frac{1}{N}\sum_{m=1}^Nu_m)W_2)$ where $W_2$ is a learning matrix. Once $c$ has been computed we can compute the attention weight for each node $n$. The idea is that nodes which are similar to the global context should receive higher attention weights. To ensure that the attention weights are in a range of 0 and 1 a sigmoid activation function $\sigma$ is being used\cite{Bai2018}.


The graph-level embeddings are then fed into a Neural Tensor Network to model their relation with each other, seen in \autoref{Eq:NTN}.


\begin{equation}\label{Eq:NTN}
g(h_i,h_j) = f_3(h_i^TW_3^{[1:K]}h_j+V\begin{bmatrix}h_i \\ h_j \end{bmatrix} + b_3)
\end{equation}

In \autoref{Eq:NTN} $h_i$ and $h_j$ are the two graph-level embeddings we would like to compute the relation on. $W_3^{[1:K]}$ is a weight tensor, $b_3$ is a bias,  $f_3$ is an activation function and $K$ is a hyperparameter used for controlling the number of similarity scores produced by the model for each graph-level embedding pair\cite{Bai2018}. This then produces a list of similarity scores which are then feed into a standard feedforward neural network to reduce the dimensionality of the similarity score and finally only produce a single similarity scores which is the predicted graph edit distance between the two graphs. This prediction is the compared with the expected result using the mean squared error loss function\cite{Bai2018}.


Besides computing a coarse comparison between the two graphs which is what was done in the first strategy, SimGNN also allows for supplementing with a finer node level comparison which is what the second strategy does.

The second strategy utilizes the same node-level embedding computed by the GCN algorithm as the first strategy a6nd then a set of pairwise interaction score are computed by $S = \sigma(U_iU_j^T)$ where $U_i$ and $U_j$ are node embedding for the two graphs. $S$ is also called a pairwise similarity matrix. In the case where one graph has less nodes than the other a set of fakes node are padded on. Once the $S$ has been computed we extract histogram features $hist(S)$. These histogram features are then normalized and concatenated with the graph-level interaction scores computed in the first strategy and then fed into a feed-forward neural network\cite{Bai2018}.



