\subsection{Metric Tree representation}
  Since we can assume, with good reason, that datasets used in recommender systems only will increase in size, we are interested in minimizing the computation time of structural data queries, like the k-nearest neighbor query.
  In order to find the exact k-nearest neighbors for only one user, we would have to compare the user with all the other users.
  This will result in a linear runtime, which is usually categorized as efficient in the theory of algorithm design and implementation. But since the datasets used in recommender systems tend to be very large, including millions of users, having to calculate the distance between one user and millions of other users may become quite time consuming.
  Furthermore, if each user has to be compared to any other user, a brute force k-nearest neighbor approach will lead to a quadratic runtime.
  As a solution to this problem, we can use the Metric Tree (MT) datastructure for improving nearest neighbor retrieval.
  We refer to \cite{jaeger2019counts} and \cite{uhlmann1991} for the complete definition of MTs, but we give a short explanation of the parts relevant to this paper.

  The Metric Tree is a datastructure that consists of two types of nodes, internal nodes and leaf nodes. An internal node contains two entities and two branches. A leaf node contains a set of entities, also known as the bucket.
  A MT is constructed by a recursive procedure in which a dataset is split. The procedure splits data and recursively calls itself over each of the subsets, until a stopping condition is met.
  If the maximal tree depth is reached, or the current set to be splitted is not larger than the maximal bucket size, a leaf node is returned.
  If not, two entities $z1$ and $z2$ are chosen randomly from the dataset and data are splitted according to their distances to these entities. Data that are closer to $z1$ go to the left branch, the others go to the right one, and we proceed with the recursive step by calling the procedure on each of the branches.

  With the Metric trees implemented, we can use a greedy algorithm for finding the approximate k-nearest-neighbors for a user.
  As stated in \cite{jaeger2019counts}, given a metric tree, the fastest solution for approximate k-nearest-neighbor retrieval for a query object is to traverse the tree, following at each node the branch whose corresponding entity is closer to the query one, until a leaf node is found.
  We can then sort the entities in the bucket contained in the leaf node according to their distance to the query object, and return the k nearest neighbors. We note here that this only is an approximate solution, but it will provide us with results that are more than accurate for recommendation tasks.
