\section{Related work}
\label{sec:Related_work}
Graph similarity is a large area of study with lots of research going on. In this section we will cover various articles related to our own study.
In the article "Page Ranking Algorithms: A Survey" N. Duhan Et. Al.\cite{4809246} describes the PageRank algorithm related concept. The authors explain strengths and weaknesses with each stage the algorithm has been through. The original PageRank algorithm are easy to implement and is the basis of PageRank. Weighted PageRank can be more precise and runs faster by having a look at what input and output a page has.

"SimRank: A Measure of Structural-Context Similarity" written by G. Jeh and J. Widom\cite{10.1145/775047.775126} describes the SimRank algorithm and its uses on graphs. In SimRank two objects are similar if they are referenced by similar objects and an object is maximally similar to itself. This leads to similarity in different part of a graph to be hard to find and it uses vast amounts of memory $(O(n^2))$. Sometimes the ranking can seem illogical to a human, but SimRank is reliable in many cases of finding similar subgraphs.

"A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications" by H. cai Et. Al.\cite{8294302} is a paper that summarizes and describes the terminology in graph embedding. They describe what an input graph can be and what kind of embedding its mostly used for. The embedding methods are plentiful and are often made to conserve data in compact graphs. The embedding method results in one of these type of embedded graphs, node-, edge-, subgraph- or whole graph embedding.

"Type Extension Trees for feature construction and learning in relational domains" by M. Jaeger Et. Al. \cite{JAEGER201330} defines framework called Type-Extension-Trees (TETs) which are used to represent complex counts-of-counts features which are complex combinatorial data structures in a relational domain. They present a learning algorithm which can be used to discover informative counts-of-counts features. These TET features can be made into prediction models either via the use of simple discriminant functions which allows for binary classification or via the use of a metric on TET values based on the Wasserstein-Kantorovich metric which allows for distance-based methods.

The article "Counts-of-counts similarity for prediction and search in relational data" by M. Jaeger Et. Al.\cite{jaeger2019counts}  gives a quick description of Type Extension Trees (TET) and Counts-of-Counts. The article uses the principles from "Type Extension Trees for feature construction and learning in relational domains" in forming the TETs and found substructures in databases. They showed that it is possible to calculate similarity using logical evaluation and histograms or by using earth mover's distance (EMD).

"Knowledge Graph Embedding for Link Prediction and Triplet Classification" an article by E. Shijia Et. Al. \cite{10.1007/978-981-10-3168-7_23} combats the problem of embedding knowledge graphs where graph embedding mostly has been used on social network graphs before.  Their solution is to learn semantic relations between entities in a form of edge embedding where they later would be able to ask logical questions and get one or more relations.

In "Semi-supervised Graph Embedding for Multi-label Graph Node Classification" by K. Gao Et. Al.\cite{10.1007/978-3-030-34223-4_35} they train a Graph convolution network to classify and embed subgraphs. They use a semi supervised technique because of the inconsistencies that can be found in multi-label graphs. This technique allows for unavailable relations to be represented, but since these relations are not part of the original graph, a "supervised learning only" strategy cannot be applied.
