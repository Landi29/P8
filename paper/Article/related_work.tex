\section{Related work}
\label{sec:Related_work}
Graph similarity is a large area of study with lots of research going on. In this section we will cover various articles related to our own study.

"SimRank: A Measure of Structural-Context Similarity" written by G. Jeh and J. Widom\cite{10.1145/775047.775126} describes the SimRank algorithm and its uses on graphs. 
In SimRank two objects are similar if they are referenced by similar objects and an object is maximally similar to itself. 
This leads to similarity in different parts of a graph which can be hard to find and uses vast amounts of memory $(O(n^2))$. SimRank is reliable in many cases of finding similar subgraphs.

"A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications" by H. Cai et al.\cite{8294302} summarizes and describes the terminology in graph embedding. 
They formulate what an input graph can be and what kind of embedding it is mostly used for.
 The embedding methods are plentiful and are often made to conserve data in compact graphs. 
The methods can output different types of embeddings, being node-, edge-, subgraph- or whole graph embedding, which conserve distinct aspects of the original graph. 
These different ideas of input and output can help with a general sense of terminology for the different things, but also how these different embedding methods can be used.

"Type Extension Trees for feature construction and learning in relational domains" by M. Jaeger et al. \cite{JAEGER201330} defines a framework called Type-Extension-Trees (TETs) which are used to represent complex counts-of-counts features, these features are combinatorial data structures in a relational domain. 
They present a learning algorithm that can be used to discover informative counts-of-counts features. 
These TET features can be made into prediction models either via the use of simple discriminant functions, which allows for binary classification or via the use of a metric on TET values, based on the Wasserstein-Kantorovich metric, where distance-based methods can be used. From this article we take the ideas of the Type Extension Trees as a possible method for doing structural similarity.
The reason for this is that TETs are an alternative to the embedding methods shown in \cite{8294302} and is therefore a very interesting candidate. 
TETs have been found to be good at finding strutural similarity and is therefore an alternative to embedding but the research into how useful this method would be for recommender systems have so far gone un-researched, and could prove to be even better than embedding methods.

The article "Counts-of-counts similarity for prediction and search in relational data" by M. Jaeger et al.\cite{jaeger2019counts} gives a quick description of Type Extension Trees (TET) and Counts-of-Counts. 
The article uses the principles from "Type Extension Trees for feature construction and learning in relational domains" in forming the TETs and found substructures in databases. 
They show that it is possible to calculate similarity using logical evaluation and histograms or by using earth mover's distance (EMD). 
These methods are shown to be a substantially better than current state-of-the-art graph kernels which means that it might show great strides within the subject of recommender systems through structural similarity.

"Knowledge Graph Embedding for Link Prediction and Triplet Classification" an article by E. Shijia et al. \cite{10.1007/978-981-10-3168-7_23} combats the problem of embedding knowledge graphs where graph embedding mostly has been used on social network graphs. 
Their solution is to learn semantic relations between entities in a form of edge embedding, where they later would be able to ask logical questions and get one or more relations. 
A question could be if there is a set of relations $R$ between $h$ and $t$ ($relation(h,R,t)$) or which set of tails $T$ will the relation $r$ starting at head $h$ return ($relation(h,r,T)$). 
This could be an interesting way of conducting the prediction part of the recommender system, where a pre-built graph is used and then these logical questions would become a prediction method.

In "Semi-supervised Graph Embedding for Multi-label Graph Node Classification" by K. Gao et al.\cite{10.1007/978-3-030-34223-4_35} they train a Graph Convolution Network to classify and embed subgraphs. 
They use a semi supervised technique because of the inconsistencies that can be found in multi-label graphs. 
This technique allows for unavailable relations to be represented, but since these relations are not part of the original graph, a "supervised learning only" strategy cannot be applied. 
Graph Convolution Network is shown to be very good at embedding subgraphs and could be applied in this article's use case.
These networks could be interesting to check if they can be used in a recommender method, that uses these embeddings to make its predictions.
